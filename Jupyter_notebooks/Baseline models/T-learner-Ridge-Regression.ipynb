{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-learner Ridge Regression baseline for IHDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class flags:\n",
    "    \n",
    "    x_dim = 25\n",
    "    y_dim = 1\n",
    "    t_dim = 2\n",
    "    \n",
    "    M = 30\n",
    "    \n",
    "    # optimization\n",
    "    learning_rate = 1e-3 # Base learning rate\n",
    "    lr_decay = 0.999995 # Learning rate decay, applied every step of the optimization\n",
    "    \n",
    "    batch_size = 128 # Batch size during training per GPU\n",
    "    hidden_size = 2\n",
    "    \n",
    "    \n",
    "FLAGS = flags()\n",
    "args = FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     3,
     12,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def int_shape(x):\n",
    "    return list(map(int, x.get_shape()))\n",
    "\n",
    "def print_shape(x,varname='variable'):\n",
    "    if x is None:\n",
    "        print('%s size: None' % (varname))\n",
    "        return\n",
    "    x_shape = x.shape.as_list()\n",
    "    # print('%s size: [%d,%d,%d]' % (varname,x_shape[1],x_shape[2],x_shape[3]))\n",
    "    print(varname,end=': ')\n",
    "    print(x_shape)\n",
    "\n",
    "def tf_eval(tf_tensor,n_samples,feed_dict=None):\n",
    "    \n",
    "    MLOOP = np.int(np.ceil(n_samples/FLAGS.batch_size))\n",
    "    \n",
    "    dd = tf_tensor.shape.as_list()[1:]\n",
    "    dd.insert(0,n_samples)\n",
    "    \n",
    "    x = np.zeros(dd)\n",
    "    \n",
    "    for mloop in range(MLOOP):\n",
    "        \n",
    "        st = mloop*FLAGS.batch_size\n",
    "        ed = min((mloop+1)*FLAGS.batch_size, n_samples)\n",
    "        \n",
    "        if feed_dict is not None:\n",
    "            feed_dict_i = dict()\n",
    "            for key in feed_dict.keys():\n",
    "                feed_dict_i[key] = np.random.randn(*int_shape(key))\n",
    "                feed_dict_i[key][:ed-st] = feed_dict[key][st:ed]\n",
    "            y = sess.run(tf_tensor,feed_dict=feed_dict_i)\n",
    "        else:\n",
    "            y = sess.run(tf_tensor)\n",
    "        \n",
    "        # print([st,ed])\n",
    "        x[st:ed] = y[:ed-st]\n",
    "    \n",
    "    return x\n",
    "\n",
    "def tf_eval_list(tf_tensor_list,n_samples,feed_dict=None):\n",
    "    \n",
    "    if isinstance(tf_tensor_list, list)==False:\n",
    "        print('Input not a list')\n",
    "        return None\n",
    "    \n",
    "    MLOOP = np.int(np.ceil(n_samples/FLAGS.batch_size))\n",
    "    \n",
    "    res = dict()\n",
    "\n",
    "    for key in tf_tensor_list:\n",
    "        dd = key.shape.as_list()[1:]\n",
    "        dd.insert(0,n_samples)\n",
    "        res[key] = np.zeros(dd)\n",
    "    \n",
    "    for mloop in range(MLOOP):\n",
    "        \n",
    "        st = mloop*FLAGS.batch_size\n",
    "        ed = min((mloop+1)*FLAGS.batch_size,n_samples)\n",
    "        \n",
    "        if feed_dict is not None:\n",
    "            feed_dict_i = dict()\n",
    "            for key in feed_dict.keys():\n",
    "                feed_dict_i[key] = np.random.randn(*int_shape(key))\n",
    "                feed_dict_i[key][:ed-st] = feed_dict[key][st:ed]\n",
    "            # print(feed_dict_i)\n",
    "            y = sess.run(tf_tensor_list,feed_dict=feed_dict_i)\n",
    "        else:\n",
    "            y = sess.run(tf_tensor_list)\n",
    "        \n",
    "        for i in range(len(tf_tensor_list)):\n",
    "            res[tf_tensor_list[i]][st:ed] = y[i][:ed-st]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     13
    ]
   },
   "outputs": [],
   "source": [
    "def simple_mlp(x,out_dim,name):\n",
    "    \n",
    "    hidden_units = 64   # size of hidden units in a layer\n",
    "    \n",
    "    input_tensor = x\n",
    "    \n",
    "    with tf.variable_scope('%s' % name,reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,activation=tf.nn.relu)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,activation=tf.nn.relu)\n",
    "        o = tf.layers.dense(h2,out_dim,activation=None)\n",
    "        \n",
    "    return o;\n",
    "\n",
    "def linear_mdl(x,out_dim,name):\n",
    "    \n",
    "    with tf.variable_scope('%s' % name,reuse=tf.AUTO_REUSE):\n",
    "        o = tf.layers.dense(x,out_dim,activation=None)\n",
    "        \n",
    "    return o;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_pehe(tau_hat,tau):\n",
    "    return np.sqrt(np.mean(np.square(tau-tau_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def onehot(t,dim):\n",
    "    \n",
    "    m_samples = t.shape[0]\n",
    "    tt = np.zeros([m_samples,dim])\n",
    "    \n",
    "    for i in range(m_samples):\n",
    "        tt[i,np.int(t[i])] = 1\n",
    "        \n",
    "    return tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_ihdp(trial_id=0,filepath='./data/',istrain=True):\n",
    "    \n",
    "    if istrain:\n",
    "        data_file = filepath+'ihdp_npci_1-1000.train.npz'\n",
    "    else:\n",
    "        data_file = filepath+'ihdp_npci_1-1000.test.npz'\n",
    "        \n",
    "    data = np.load(data_file)\n",
    "    \n",
    "    x = data['x'][:,:,trial_id]\n",
    "    y = data['yf'][:,trial_id]\n",
    "    t = data['t'][:,trial_id]\n",
    "    ycf = data['ycf'][:,trial_id]\n",
    "    mu0 = data['mu0'][:,trial_id]\n",
    "    mu1 = data['mu1'][:,trial_id]\n",
    "    \n",
    "    return x,y,t,ycf,mu0,mu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ihdp_ind = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------\n",
    "# \n",
    "#     LOAD DATA: ihdp data\n",
    "#\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "NORM = True\n",
    "VAL = True\n",
    "\n",
    "# load trainning dataset\n",
    "#------------------------------------------------\n",
    "\n",
    "X,Y,T,Ycf,Mu0,Mu1 = load_ihdp(ihdp_ind)\n",
    "Tau = Mu1 - Mu0\n",
    "\n",
    "Y = np.reshape(Y,[-1,1])\n",
    "T = onehot(T,2)\n",
    "\n",
    "X_m = np.mean(X,axis=0,keepdims=True)\n",
    "X_std = np.std(X,axis=0,keepdims=True)\n",
    "X = (X-X_m)/X_std\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "\n",
    "y_std = 1.\n",
    "\n",
    "if NORM:\n",
    "    y_m = np.mean(Y)\n",
    "    y_std = np.std(Y)\n",
    "\n",
    "    Y = (Y-y_m)/y_std\n",
    "\n",
    "    Tau = Tau/y_std\n",
    "    \n",
    "\n",
    "# split trainning dataset for CV\n",
    "#-------------------------------\n",
    "\n",
    "if VAL:\n",
    "\n",
    "    prob_train = 0.7\n",
    "\n",
    "    n_train_samples = int(np.ceil(prob_train*n_samples))\n",
    "\n",
    "    shuff_idx = np.array(range(n_samples))\n",
    "    # Shuffle the indices\n",
    "    # np.random.shuffle(shuff_idx)\n",
    "\n",
    "    train_idx = shuff_idx[:n_train_samples]\n",
    "    val_idx = shuff_idx[n_train_samples:]\n",
    "\n",
    "    X_val = X[val_idx]\n",
    "    Y_val = Y[val_idx]\n",
    "    T_val = T[val_idx]\n",
    "\n",
    "    X = X[train_idx]\n",
    "    Y = Y[train_idx]\n",
    "    T = T[train_idx]\n",
    "\n",
    "    n_samples = n_train_samples\n",
    "\n",
    "    Tau_val = Tau[val_idx]\n",
    "    Tau = Tau[train_idx]    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# data formating\n",
    "#----------------\n",
    "t1_ind = T[:,1]==1   # find which column has the treatment == 1\n",
    "t0_ind = T[:,0]==1 \n",
    "\n",
    "n0 = np.sum(t0_ind)\n",
    "n1 = np.sum(t1_ind)\n",
    "\n",
    "X0 = X[t0_ind]\n",
    "X1 = X[t1_ind]\n",
    "\n",
    "Y0 = Y[t0_ind]\n",
    "Y1 = Y[t1_ind]\n",
    "\n",
    "\n",
    "t1_ind = T_val[:,1]==1   # find which column has the treatment == 1\n",
    "t0_ind = T_val[:,0]==1 \n",
    "\n",
    "n0 = np.sum(t0_ind)\n",
    "n1 = np.sum(t1_ind)\n",
    "\n",
    "X0_val = X_val[t0_ind]\n",
    "X1_val = X_val[t1_ind]\n",
    "\n",
    "Y0_val = Y_val[t0_ind]\n",
    "Y1_val = Y_val[t1_ind]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load testing dataset \n",
    "#------------------------------------------------\n",
    "\n",
    "# load testing dataset\n",
    "X_test,Y_test,T_test,Ycf_test,Mu0_test,Mu1_test = load_ihdp(ihdp_ind,istrain=False)\n",
    "\n",
    "\n",
    "Tau_test = Mu1_test - Mu0_test\n",
    "Y_test = np.reshape(Y_test,[-1,1])\n",
    "\n",
    "if NORM:\n",
    "\n",
    "    Y_test  = (Y_test -y_m )/y_std\n",
    "    Tau_test  = Tau_test /y_std\n",
    "\n",
    "\n",
    "T_test = onehot(T_test,2)\n",
    "# Tau.shape\n",
    "# np.mean(Tau)\n",
    "# X.shape\n",
    "\n",
    "\n",
    "X_test = (X_test-X_m)/X_std\n",
    "\n",
    "n_samples_test = X_test.shape[0]\n",
    "\n",
    "            \n",
    "t1_ind_test = T_test[:,1]==1   # find which column has the treatment == 1\n",
    "t0_ind_test = T_test[:,0]==1 \n",
    "\n",
    "n0_test = np.sum(t0_ind_test)\n",
    "n1_test = np.sum(t1_ind_test)\n",
    "\n",
    "X0_test = X_test[t0_ind_test]\n",
    "X1_test = X_test[t1_ind_test]\n",
    "\n",
    "Y0_test = Y_test[t0_ind_test]\n",
    "Y1_test = Y_test[t1_ind_test]\n",
    "\n",
    "\n",
    "Y = Y.reshape([-1,])\n",
    "Y_val = Y_val.reshape([-1,])\n",
    "Y_test = Y_test.reshape([-1,])\n",
    "\n",
    "Y0 = Y0.reshape([-1,])\n",
    "Y1 = Y1.reshape([-1,])\n",
    "\n",
    "Y0_val = Y0_val.reshape([-1,])\n",
    "Y1_val = Y1_val.reshape([-1,]) \n",
    "\n",
    "Y0_test = Y0_test.reshape([-1,])\n",
    "Y1_test = Y1_test.reshape([-1,]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(T,axis=0)\n",
    "# np.sum(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def mu_learner(x,t):\n",
    "    \n",
    "    input_tensor = tf.concat([x,tf.cast(t,tf.float32)],axis=-1) \n",
    "    \n",
    "    mu = simple_mlp(input_tensor,FLAGS.y_dim,'mu')\n",
    "    \n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_ols(X,Y,X_val,Y_val,X_all,X_val_all,X_test,alpha_list):\n",
    "    \n",
    "    best_rmse = np.inf\n",
    "    best_mdl = None\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        model = Ridge(alpha=alpha)\n",
    "        model.fit(X,Y)\n",
    "        Y_val_hat = model.predict(X_val)\n",
    "        rmse = np.sqrt(np.mean(np.square(Y_val_hat-Y_val)))\n",
    "        if rmse<best_rmse:\n",
    "            best_mdl = model\n",
    "            best_rmse = rmse\n",
    "    \n",
    "    Y_hat = best_mdl.predict(X_all)\n",
    "    Y_val_hat = best_mdl.predict(X_val_all)\n",
    "    Y_test_hat = best_mdl.predict(X_test)\n",
    "    \n",
    "    return Y_hat,Y_val_hat,Y_test_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [1e-5,1e-4,1e-3,1e-2,1e-1,1,1e1,1e2,1e3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu0_hat,mu0_val_hat,mu0_test_hat = get_best_ols(X0,Y0,X0_val,Y0_val,X,X_val,X_test,alpha_list)\n",
    "mu1_hat,mu1_val_hat,mu1_test_hat = get_best_ols(X1,Y1,X1_val,Y1_val,X,X_val,X_test,alpha_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5286407483508259, 0.5735575272566845, 0.5497413527907465]\n"
     ]
    }
   ],
   "source": [
    "tau_hat = mu1_hat - mu0_hat\n",
    "tau_val_hat= mu1_val_hat - mu0_val_hat\n",
    "tau_test_hat= mu1_test_hat - mu0_test_hat\n",
    "\n",
    "pehe_ls = eval_pehe(tau_hat, Tau)*y_std\n",
    "pehe_val_ls = eval_pehe(tau_val_hat, Tau_val)*y_std\n",
    "pehe_test_ls = eval_pehe(tau_test_hat, Tau_test)*y_std\n",
    "\n",
    "# print(pehe_ls)\n",
    "print([pehe_ls,pehe_val_ls,pehe_test_ls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.plot(Tau,tau_hat,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(tau_hat,bins=20,range=[-1,5])\n",
    "_ = plt.hist(Tau,bins=20,range=[-1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu_hat = tf_eval(mu,n_samples,{input_x: X, input_t: T})\n",
    "mu_hat_t0 = tf_eval(mu0,n0,{input_x_t0: X0})\n",
    "mu_hat_t1 = tf_eval(mu1,n1,{input_x_t1: X1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = plt.plot(Y,mu_hat,'s')\n",
    "_ = plt.plot(Y0,mu_hat_t0,'s')\n",
    "_ = plt.plot(Y1,mu_hat_t1,'s')\n",
    "\n",
    "_ = plt.legend(['T=0','T=1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
