{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\ELBO}{\\text{ELBO}}\n",
    "\\newcommand{\\EE}{\\mathbb{E}}\n",
    "\\newcommand{\\bs}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\ud}{\\,\\text{d}}\n",
    "\\newcommand{\\CN}{\\mathcal{N}}\n",
    "\\newcommand{\\CB}{\\mathcal{B}}\n",
    "\\newcommand{\\aux}{\\text{aux}}\n",
    "\\newcommand{\\do}{do}\n",
    "\\newcommand{\\Xmat}{\\boldsymbol{X}}\n",
    "\\newcommand{\\zv}{\\bs{z}}\n",
    "\\newcommand{\\xv}{\\bs{x}}\n",
    "\\newcommand{\\Xv}{\\bs{X}}\n",
    "\\newcommand{\\Yv}{\\bs{Y}}\n",
    "\\newcommand{\\BD}{\\mathbb{D}}\n",
    "\\newcommand{\\KL}{\\text{KL}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing Variational Neural Inference Causal Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using reformulated ELBO for training\n",
    "  * $m(\\zv) = \\EE[Y|\\zv]$: expected outcome model\n",
    "  * $e(\\zv) = p(t|\\zv)$: propensity score model\n",
    "  * $\\tau(\\zv) = \\EE[Y(1) - Y(0)|\\zv]$: causal effect model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Learning objective:\n",
    "  * Robinson residual: $r(\\zv) = y - m(\\zv) - (t - e(\\zv))\\times\\tau(\\zv)$\n",
    "  * $\\ELBO = \\log e(\\zv) - \\rho r^2(\\zv) - \\BD_{\\KL}(q \\parallel p) - \\lambda \\BD_{\\KL}(q_0 \\parallel q_1)$\n",
    "    * $\\zv \\sim q(\\zv|\\xv,y,t)$\n",
    "    * $\\rho$ is the precision parameter\n",
    "    * $\\lambda$ is the balancing regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# import time\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l2_reg = 1e-4\n",
    "beta = 1.   # ELBO's KL term\n",
    "gamma = 1.  # PS likelihood\n",
    "rho = 1     # outcome likelihood 5\n",
    "lam = 1     # imbalance parameter\n",
    "\n",
    "lr = 1e-4\n",
    "# lam = 0\n",
    "\n",
    "ihdp_id = 0\n",
    "# rho = 10\n",
    "\n",
    "# NORM = False\n",
    "NORM = True\n",
    "\n",
    "l2_reg = 0\n",
    "# l2_reg = 1e-3\n",
    "\n",
    "# L2REG = True\n",
    "# L2REG = False\n",
    "\n",
    "# RDECOMP = False\n",
    "RDECOMP = True\n",
    "\n",
    "# AVB = False\n",
    "AVB = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class flags:\n",
    "    \n",
    "    # dim = 2\n",
    "    \n",
    "    x_dim = 25\n",
    "    y_dim = 1\n",
    "    t_dim = 2\n",
    "    M = 10\n",
    "    \n",
    "    hidden_units = 64\n",
    "    hidden_size = 2\n",
    "    \n",
    "    xi_dim = 100\n",
    "    \n",
    "    # optimization\n",
    "    learning_rate = 1e-3 # Base learning rate\n",
    "    lr_decay = 0.999995 # Learning rate decay, applied every step of the optimization\n",
    "    \n",
    "    batch_size = 128 # Batch size during training per GPU\n",
    "    # batch_size = 400\n",
    "    \n",
    "    \n",
    "FLAGS = flags()\n",
    "args = FLAGS\n",
    "\n",
    "DTYPE = tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(t,dim):\n",
    "    \n",
    "    m_samples = t.shape[0]\n",
    "    tt = np.zeros([m_samples,dim])\n",
    "    \n",
    "    for i in range(m_samples):\n",
    "        tt[i,np.int(t[i])] = 1\n",
    "        \n",
    "    return tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    y = exp_x / np.sum(exp_x,axis=1,keepdims=True)\n",
    "    # y = y / np.sum(y,axis=1,keepdims=True)\n",
    "    return y\n",
    "\n",
    "def sample_category_logits(logits):\n",
    "    pval = softmax(logits)\n",
    "    n,m = pval.shape\n",
    "    sampl = np.zeros([n,m])\n",
    "    for i in range(pval.shape[0]):\n",
    "        sampl[i,:] = np.random.multinomial(1,pval[i,:])\n",
    "\n",
    "    return sampl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?tf.layers.dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     6,
     37,
     58,
     78,
     97
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "# initializer = None\n",
    "\n",
    "# nonlinearity=tf.nn.elu\n",
    "nonlinearity=tf.nn.relu\n",
    "\n",
    "def encoder(input_x,input_y,input_t,name='encoder'):\n",
    "    '''\n",
    "    approximate posterior of z given x,y,t\n",
    "    p(z|x,y,t)\n",
    "    return mean and var of z\n",
    "    \n",
    "    '''\n",
    "    input_tensor = tf.concat([input_x,input_y,input_t],axis=-1)  # cbind \n",
    "    \n",
    "    hidden_units = FLAGS.hidden_units   # size of hidden units in a layer\n",
    "    # latent_size = 2     # dimension of z\n",
    "    \n",
    "    with tf.variable_scope(name,reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        # h2 = tf.layers.dense(h2,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        o = tf.layers.dense(h2,2*FLAGS.hidden_size,kernel_initializer=initializer,activation=None)\n",
    "        # o = tf.layers.dense(h2,2*latent_size,activation=None,use_bias=False)\n",
    "        \n",
    "    # o is the output of encoder\n",
    "    # o[:hidden_size]: mean(z)\n",
    "    # o[hidden_size:]: log var(z)\n",
    "    \n",
    "#     mean = o[:, :FLAGS.hidden_size]\n",
    "#     std = o[:, FLAGS.hidden_size:]\n",
    "#     o = tf.concat([mean,tf.constant(-4+np.zeros([FLAGS.batch_size,FLAGS.hidden_size]),tf.float32)],axis=1)\n",
    "    \n",
    "    # stddev = tf.sqrt(tf.exp(latent_code[:, FLAGS.hidden_size:]))\n",
    "    \n",
    "    return o\n",
    "\n",
    "def s_encoder(input_x,input_y,input_t,name='encoder'):\n",
    "    '''\n",
    "    stocahstic encoder\n",
    "    approximate posterior of z given x,y,t\n",
    "    p(z|x,y,t)\n",
    "    \n",
    "    '''\n",
    "    xi = tf.random.uniform([FLAGS.batch_size,FLAGS.xi_dim], minval=-1,maxval=1)\n",
    "    input_tensor = tf.concat([input_x,input_y,input_t,xi],axis=-1)  # cbind \n",
    "    \n",
    "    hidden_units = FLAGS.hidden_units   # size of hidden units in a layer\n",
    "    # latent_size = 2     # dimension of z\n",
    "    \n",
    "    with tf.variable_scope(name,reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        # h2 = tf.layers.dense(h2,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        o = tf.layers.dense(h2,FLAGS.hidden_size,kernel_initializer=initializer,activation=None)\n",
    "        \n",
    "    return o\n",
    "\n",
    "def generator(x, name='generator'):\n",
    "    \n",
    "    '''\n",
    "    Conditional generator for q(z|x)\n",
    "    '''\n",
    "    \n",
    "    hidden_units = FLAGS.hidden_units \n",
    "    \n",
    "    xi = tf.random.uniform([FLAGS.batch_size,FLAGS.xi_dim], minval=-1,maxval=1)\n",
    "    \n",
    "    input_tensor = tf.concat([x,xi],axis=-1)\n",
    "    \n",
    "    with tf.variable_scope('%s' % name,reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        # h2 = tf.layers.dense(h2,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        o = tf.layers.dense(h2,FLAGS.hidden_size,kernel_initializer=initializer,activation=None)\n",
    "    \n",
    "    return o\n",
    "    \n",
    "def critic(z,x,name):\n",
    "    \n",
    "    hidden_units = FLAGS.hidden_units    # size of hidden units in a layer\n",
    "    \n",
    "    if x is None:\n",
    "        input_tensor = z;\n",
    "    else:\n",
    "        input_tensor = tf.concat([x,z],axis=-1)\n",
    "    \n",
    "    with tf.variable_scope('critic-%s' % name,reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        # h2 = tf.layers.dense(h2,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        # o = tf.layers.dense(h2,1,activation=None)\n",
    "        o = tf.layers.dense(h2,1,kernel_initializer=initializer,activation=tf.nn.tanh)\n",
    "        o *= 5\n",
    "        \n",
    "    return o;\n",
    "\n",
    "def encoder2(input_x,input_y,input_t,name='encoder',nonlinearity=tf.nn.elu):\n",
    "    '''\n",
    "    approximate posterior of z given x,y,t\n",
    "    p(z|x,y,t)\n",
    "    return mean and var of z\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if input_y is None or input_t is None:\n",
    "        input_tensor = input_x\n",
    "    else:\n",
    "        input_tensor = tf.concat([input_x,input_y,input_t],axis=-1)  # cbind \n",
    "    \n",
    "    hidden_units = FLAGS.hidden_units   # size of hidden units in a layer\n",
    "    # latent_size = 2     # dimension of z\n",
    "    \n",
    "    with tf.variable_scope(name,reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        # h2 = tf.layers.dense(h2,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        o = tf.layers.dense(h2,2*FLAGS.hidden_size,kernel_initializer=initializer,activation=None)\n",
    "        # o = tf.layers.dense(h2,2*latent_size,activation=None,use_bias=False)\n",
    "        \n",
    "    # o is the output of encoder\n",
    "    # o[:hidden_size]: mean(z)\n",
    "    # o[hidden_size:]: log var(z)\n",
    "    \n",
    "    return o,h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def simple_mlp(x,out_dim,name):\n",
    "    \n",
    "    hidden_units = FLAGS.hidden_units   # size of hidden units in a layer\n",
    "    \n",
    "    input_tensor = x\n",
    "    \n",
    "    with tf.variable_scope('%s' % name,reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        # h2 = tf.layers.dense(h2,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        o = tf.layers.dense(h2,out_dim,kernel_initializer=initializer,activation=None)\n",
    "        \n",
    "    return o;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     20
    ]
   },
   "outputs": [],
   "source": [
    "def qt(input_x,nonlinearity=tf.nn.elu):\n",
    "    '''\n",
    "    Auxiliary treatment predictor\n",
    "    q(t|x)\n",
    "\n",
    "    return: logits\n",
    "    '''\n",
    "\n",
    "    input_tensor = input_x\n",
    "\n",
    "    hidden_units = 64   # size of hidden units in a layer\n",
    "    # latent_size = 2     # dimension of z\n",
    "\n",
    "    with tf.variable_scope('qt',reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,activation=nonlinearity)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,activation=nonlinearity)\n",
    "        o = tf.layers.dense(h2,2,activation=None)\n",
    "\n",
    "    return o\n",
    "\n",
    "def qy(input_x,input_t,nonlinearity=tf.nn.elu):\n",
    "    '''\n",
    "    Auxiliary outcome predictor\n",
    "    q(y|x,t)\n",
    "\n",
    "    return: logits\n",
    "    '''\n",
    "\n",
    "    input_tensor = tf.concat([input_x,input_t],axis=-1)\n",
    "\n",
    "    hidden_units = 64   # size of hidden units in a layer\n",
    "    # latent_size = 2     # dimension of z\n",
    "\n",
    "    with tf.variable_scope('qy',reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,activation=nonlinearity)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,activation=nonlinearity)\n",
    "        o = tf.layers.dense(h2,FLAGS.M,activation=None)\n",
    "\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def eval_pehe(tau_hat,tau):\n",
    "    return np.sqrt(np.mean(np.square(tau-tau_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAE utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_kl_cost(mean,stddev, epsilon=1e-8):                      \n",
    "    '''\n",
    "    Compute the KL-divergence btw q(z|x) and p(z)\n",
    "    \n",
    "    KL(q||p)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return tf.reduce_sum(0.5 * (tf.square(mean) + tf.square(stddev) -\n",
    "                                2.0 * tf.log(stddev + epsilon) - 1.0), axis=1)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0,
     3,
     12,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def int_shape(x):\n",
    "    return list(map(int, x.get_shape()))\n",
    "\n",
    "def print_shape(x,varname='variable'):\n",
    "    if x is None:\n",
    "        print('%s size: None' % (varname))\n",
    "        return\n",
    "    x_shape = x.shape.as_list()\n",
    "    # print('%s size: [%d,%d,%d]' % (varname,x_shape[1],x_shape[2],x_shape[3]))\n",
    "    print(varname,end=': ')\n",
    "    print(x_shape)\n",
    "\n",
    "def tf_eval(tf_tensor,n_samples,feed_dict=None):\n",
    "    \n",
    "    MLOOP = np.int(np.ceil(n_samples/FLAGS.batch_size))\n",
    "    \n",
    "    dd = tf_tensor.shape.as_list()[1:]\n",
    "    dd.insert(0,n_samples)\n",
    "    \n",
    "    x = np.zeros(dd)\n",
    "    \n",
    "    for mloop in range(MLOOP):\n",
    "        \n",
    "        st = mloop*FLAGS.batch_size\n",
    "        ed = min((mloop+1)*FLAGS.batch_size, n_samples)\n",
    "        \n",
    "        if feed_dict is not None:\n",
    "            feed_dict_i = dict()\n",
    "            for key in feed_dict.keys():\n",
    "                feed_dict_i[key] = np.random.randn(*int_shape(key))\n",
    "                feed_dict_i[key][:ed-st] = feed_dict[key][st:ed]\n",
    "            y = sess.run(tf_tensor,feed_dict=feed_dict_i)\n",
    "        else:\n",
    "            y = sess.run(tf_tensor)\n",
    "        \n",
    "        # print([st,ed])\n",
    "        x[st:ed] = y[:ed-st]\n",
    "    \n",
    "    return x\n",
    "\n",
    "def tf_eval_list(tf_tensor_list,n_samples,feed_dict=None):\n",
    "    \n",
    "    if isinstance(tf_tensor_list, list)==False:\n",
    "        print('Input not a list')\n",
    "        return None\n",
    "    \n",
    "    MLOOP = np.int(np.ceil(n_samples/FLAGS.batch_size))\n",
    "    \n",
    "    res = dict()\n",
    "\n",
    "    for key in tf_tensor_list:\n",
    "        dd = key.shape.as_list()[1:]\n",
    "        dd.insert(0,n_samples)\n",
    "        res[key] = np.zeros(dd)\n",
    "    \n",
    "    for mloop in range(MLOOP):\n",
    "        \n",
    "        st = mloop*FLAGS.batch_size\n",
    "        ed = min((mloop+1)*FLAGS.batch_size,n_samples)\n",
    "        \n",
    "        if feed_dict is not None:\n",
    "            feed_dict_i = dict()\n",
    "            for key in feed_dict.keys():\n",
    "                feed_dict_i[key] = np.random.randn(*int_shape(key))\n",
    "                feed_dict_i[key][:ed-st] = feed_dict[key][st:ed]\n",
    "            # print(feed_dict_i)\n",
    "            y = sess.run(tf_tensor_list,feed_dict=feed_dict_i)\n",
    "        else:\n",
    "            y = sess.run(tf_tensor_list)\n",
    "        \n",
    "        for i in range(len(tf_tensor_list)):\n",
    "            res[tf_tensor_list[i]][st:ed] = y[i][:ed-st]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [],
   "source": [
    "def ymul2y(yy,Yq):\n",
    "    \n",
    "    m_samples = yy.shape[0]\n",
    "    y = np.zeros([m_samples,1])\n",
    "    \n",
    "    for i in range(m_samples):\n",
    "        j = np.where(yy[i,:])[0][0]\n",
    "        y[i] = Yq[j] + np.random.rand()*(Yq[j+1]-Yq[j]) \n",
    "        \n",
    "    return y\n",
    "\n",
    "def estimate_causal_effect(xx,n_runs=1,progress=False):\n",
    "    \n",
    "    m_samples = xx.shape[0]\n",
    "    tau_sum = np.zeros([m_samples,1])\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        \n",
    "        if progress:\n",
    "            print('Computing %d / %d ...' % (i+1, n_runs))\n",
    "        \n",
    "        # zz = tf_eval(sampled_z_x, m_samples, {input_x: xx})\n",
    "        # tau_val = tf_eval(tau_z, m_samples, {sampled_z: zz})\n",
    "        tau_val = tf_eval(tau_z_x, m_samples, {input_x: xx})\n",
    "        \n",
    "        tau_sum += tau_val\n",
    "        \n",
    "    return tau_sum/n_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0,
     28,
     34
    ]
   },
   "outputs": [],
   "source": [
    "def estimate_ps(xx,n_runs=1,progress=False):\n",
    "    \n",
    "    '''\n",
    "    xx: confounder proxies\n",
    "    to: observed treatment\n",
    "    '''\n",
    "    \n",
    "    m_samples = xx.shape[0]\n",
    "    # tau_sum = np.zeros([m_samples,1])\n",
    "    \n",
    "    # print(m_samples)\n",
    "    \n",
    "    yy_mdl = np.zeros([m_samples,n_runs])\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        \n",
    "        if progress:\n",
    "            print('Computing %d / %d ...' % (i+1, n_runs))\n",
    "        \n",
    "#         res = tf_eval_list([tau_z_x, m_z_x, e_z_x], m_samples, {input_x: xx})\n",
    "#         m_val = res[m_z_x]\n",
    "#         tau_val = res[tau_z_x]\n",
    "#         e_val = res[e_z_x]\n",
    "        e_val = tf_eval(e_z_x, m_samples, {input_x: xx})\n",
    "        yy_mdl[:,i] = e_val.reshape([-1,])\n",
    "        \n",
    "    return yy_mdl\n",
    "\n",
    "def eval_nll_ps(X_val,T_val,n_runs=1):\n",
    "    ps_val = np.mean(estimate_ps(X_val,n_runs=100),axis=1)\n",
    "    loglik_ps_val = np.log(T_val[:,0]*(1-ps_val)+T_val[:,1]*ps_val)\n",
    "    ps_nll_val = -np.mean(loglik_ps_val)\n",
    "    return ps_nll_val\n",
    "\n",
    "def eval_nll_ps_x(X_val,T_val):\n",
    "    \n",
    "    m_samples = X_val.shape[0]\n",
    "    ps_val = tf_eval(e_x, m_samples, {input_x: X_val}).reshape([-1,])\n",
    "    loglik_ps_val = np.log(T_val[:,0]*(1-ps_val)+T_val[:,1]*ps_val)\n",
    "    ps_nll_val = -np.mean(loglik_ps_val)\n",
    "    \n",
    "    return ps_nll_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_ihdp(trial_id=0,filepath='./data/',istrain=True):\n",
    "    \n",
    "    if istrain:\n",
    "        data_file = filepath+'ihdp_npci_1-1000.train.npz'\n",
    "    else:\n",
    "        data_file = filepath+'ihdp_npci_1-1000.test.npz'\n",
    "        \n",
    "    data = np.load(data_file)\n",
    "    \n",
    "    x = data['x'][:,:,trial_id]\n",
    "    y = data['yf'][:,trial_id]\n",
    "    t = data['t'][:,trial_id]\n",
    "    ycf = data['ycf'][:,trial_id]\n",
    "    mu0 = data['mu0'][:,trial_id]\n",
    "    mu1 = data['mu1'][:,trial_id]\n",
    "    \n",
    "    return x,y,t,ycf,mu0,mu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y,T,Ycf,Mu0,Mu1 = load_ihdp(ihdp_id)\n",
    "Tau = Mu1 - Mu0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.reshape(Y,[-1,1])\n",
    "\n",
    "T = onehot(T,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_m = np.mean(X,axis=0,keepdims=True)\n",
    "X_std = np.std(X,axis=0,keepdims=True)\n",
    "X = (X-X_m)/X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "\n",
    "y_std = 1.\n",
    "\n",
    "if NORM:\n",
    "    y_m = np.mean(Y)\n",
    "    y_std = np.std(Y)\n",
    "\n",
    "    Y = (Y-y_m)/y_std\n",
    "\n",
    "    Tau = Tau/y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break Y into M bins\n",
    "\n",
    "M = FLAGS.M\n",
    "q = np.linspace(0,1,M+1)\n",
    "\n",
    "Yq = np.quantile(Y,q)\n",
    "\n",
    "Yqq = Yq.copy()\n",
    "Yqq[0] = -1e10\n",
    "Yqq[-1] = 1e10\n",
    "\n",
    "n_samples = Y.shape[0]\n",
    "Ymul = np.zeros([n_samples,M])\n",
    "\n",
    "for i in range(n_samples):\n",
    "    for j in range(M):\n",
    "        if Y[i]>= Yqq[j] and Y[i]<Yqq[j+1]:\n",
    "            Ymul[i,j] = 1\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_train = 0.7\n",
    "\n",
    "n_train_samples = int(np.ceil(prob_train*n_samples))\n",
    "\n",
    "shuff_idx = np.array(range(n_samples))\n",
    "# Shuffle the indices\n",
    "# np.random.shuffle(shuff_idx)\n",
    "\n",
    "train_idx = shuff_idx[:n_train_samples]\n",
    "val_idx = shuff_idx[n_train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X[val_idx]\n",
    "Y_val = Y[val_idx]\n",
    "T_val = T[val_idx]\n",
    "\n",
    "X = X[train_idx]\n",
    "Y = Y[train_idx]\n",
    "T = T[train_idx]\n",
    "Ymul = Ymul[train_idx]\n",
    "\n",
    "n_samples = n_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tau_val = Tau[val_idx]\n",
    "Tau = Tau[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_ind = T[:,1]==1   # find which column has the treatment == 1\n",
    "t0_ind = T[:,0]==1 \n",
    "\n",
    "n0 = np.sum(t0_ind)\n",
    "n1 = np.sum(t1_ind)\n",
    "\n",
    "X0 = X[t0_ind]\n",
    "X1 = X[t1_ind]\n",
    "\n",
    "Y0 = Y[t0_ind]\n",
    "Y1 = Y[t1_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def gauss_sampler(latent_code=None):\n",
    "    epsilon = tf.random_normal([FLAGS.batch_size, FLAGS.hidden_size])\n",
    "    \n",
    "    # input_tensor = tf.cast(input_tensor,tf.float32)\n",
    "    # input_t = tf.cast(input_t, tf.float32)\n",
    "    \n",
    "    # If input is none, sample z from prior\n",
    "    # input_tensor[:half]: mean, input_tensor[half:]: log var,\n",
    "    if latent_code is None:\n",
    "        mean = None\n",
    "        stddev = None\n",
    "        input_sample = epsilon\n",
    "    else:\n",
    "        mean = latent_code[:, :FLAGS.hidden_size]\n",
    "        stddev = tf.sqrt(tf.exp(latent_code[:, FLAGS.hidden_size:]))\n",
    "        input_sample = mean + epsilon * stddev\n",
    "        \n",
    "    return input_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-54b6083475a4>:115: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "input_x = tf.placeholder(tf.float32, shape=[FLAGS.batch_size, FLAGS.x_dim])\n",
    "input_t = tf.placeholder(tf.int32, shape=[FLAGS.batch_size, FLAGS.t_dim])\n",
    "input_y = tf.placeholder(tf.float32, shape=[FLAGS.batch_size, FLAGS.y_dim])\n",
    "input_ymul = tf.placeholder(tf.int32, shape=[FLAGS.batch_size, FLAGS.M])\n",
    "\n",
    "_,phi_x = encoder2(input_x,None,None,'encoderX')\n",
    "\n",
    "if AVB:\n",
    "    sampled_z = s_encoder(phi_x, input_y, tf.cast(input_t,tf.float32))  # q(z|x)\n",
    "    \n",
    "    nu_q_vec = critic(sampled_z, None, 'ELBO')\n",
    "    nu_p_vec = critic(tf.random.normal([FLAGS.batch_size, FLAGS.hidden_size]), None, 'ELBO')\n",
    "\n",
    "    loss_kl = tf.reduce_mean(nu_q_vec) - tf.reduce_mean(tf.exp(nu_p_vec))\n",
    "    \n",
    "else:\n",
    "    latent_code = encoder(phi_x, input_y, tf.cast(input_t,tf.float32))  # q(z|x)\n",
    "    sampled_z = gauss_sampler(latent_code)\n",
    "    \n",
    "    mean = latent_code[:,:FLAGS.hidden_size]\n",
    "    stddev = tf.sqrt(tf.exp(latent_code[:,FLAGS.hidden_size:]))\n",
    "    loss_kl_vec = get_kl_cost(mean,stddev)\n",
    "    loss_kl = tf.reduce_mean(loss_kl_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\# propensity model absed on $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-32-10af06cafe52>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logit_t_x = simple_mlp(input_x, 2, 'ps_x') # propensity score\n",
    "e_x = tf.reshape(tf.nn.softmax(logit_t_x)[:,1],[-1,1])\n",
    "\n",
    "loss_e_x_vec = tf.nn.softmax_cross_entropy_with_logits(labels=input_t, logits=logit_t_x)\n",
    "loss_e_x = tf.reduce_mean(loss_e_x_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\# Robinson residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if RDECOMP:\n",
    "\n",
    "    m_z = simple_mlp(sampled_z, 1, 'm') # expected outcome \n",
    "    tau_z = simple_mlp(sampled_z, 1, 'tau') # causal effect\n",
    "    logit_t_z = simple_mlp(sampled_z, 2, 'ps') # propensity score\n",
    "    e_z = tf.reshape(tf.nn.softmax(logit_t_z)[:,1],[-1,1])\n",
    "\n",
    "    loss_e_vec = tf.nn.softmax_cross_entropy_with_logits(labels=input_t, logits=logit_t_z)\n",
    "    loss_e = tf.reduce_mean(loss_e_vec)\n",
    "\n",
    "    # Continuous outcome\n",
    "    input_t_bin = tf.reshape(tf.cast(input_t[:,1],dtype=tf.float32),[-1,1])\n",
    "    robinson_res = (input_y - m_z) - (tf.cast(input_t_bin, dtype=tf.float32) - e_z) * tau_z\n",
    "\n",
    "    loss_r = tf.reduce_mean(tf.square(robinson_res))\n",
    "\n",
    "    # loss_m = tf.reduce_mean(tf.square(input_y_bin - m_z)) # Binary outcome\n",
    "    loss_m = tf.reduce_mean(tf.square(input_y - m_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\# Regular decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RDECOMP is False:\n",
    "\n",
    "\n",
    "#     # S-learner\n",
    "#     input_zt = tf.concat([sampled_z,tf.cast(input_t,tf.float32)],axis=1)\n",
    "#     mu_t_z = simple_mlp(input_zt, 1, 'tau')\n",
    "#     t0_np = np.zeros([FLAGS.batch_size,2],dtype=np.int32); t0_np[:,0] = 1\n",
    "#     t1_np = np.zeros([FLAGS.batch_size,2],dtype=np.int32); t1_np[:,1] = 1\n",
    "#     t0_tf = tf.constant(t0_np,tf.float32)\n",
    "#     t1_tf = tf.constant(t1_np,tf.float32)\n",
    "#     input_zt0 = tf.concat([sampled_z,t0_tf],axis=1)\n",
    "#     input_zt1 = tf.concat([sampled_z,t1_tf],axis=1)\n",
    "#     mu_0_z = simple_mlp(input_zt0, 1, 'tau')\n",
    "#     mu_1_z = simple_mlp(input_zt1, 1, 'tau')\n",
    "    \n",
    "    \n",
    "    # T-learner\n",
    "    mu_0_z = simple_mlp(sampled_z, 1, 'tau0')\n",
    "    mu_1_z = simple_mlp(sampled_z, 1, 'tau1')\n",
    "    input_t_bin = tf.reshape(tf.cast(input_t[:,1],dtype=tf.float32),[-1,1])\n",
    "    mu_t_z = input_t_bin*mu_1_z + (1-input_t_bin)*mu_0_z\n",
    "    \n",
    "    \n",
    "    tau_z = mu_1_z - mu_0_z # causal effect\n",
    "\n",
    "    logit_t_z = simple_mlp(sampled_z, 2, 'ps') # propensity score\n",
    "    e_z = tf.reshape(tf.nn.softmax(logit_t_z)[:,1],[-1,1])\n",
    "\n",
    "    loss_e_vec = tf.nn.softmax_cross_entropy_with_logits(labels=input_t, logits=logit_t_z)\n",
    "    loss_e = tf.reduce_mean(loss_e_vec)\n",
    "\n",
    "    robinson_res = input_y - mu_t_z\n",
    "\n",
    "    loss_r = tf.reduce_mean(tf.square(robinson_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\# Balancing KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x_t0 = tf.placeholder(tf.float32, shape=[FLAGS.batch_size, FLAGS.x_dim])\n",
    "input_x_t1 = tf.placeholder(tf.float32, shape=[FLAGS.batch_size, FLAGS.x_dim])\n",
    "input_y_t0 = tf.placeholder(tf.float32, shape=[FLAGS.batch_size, FLAGS.y_dim])\n",
    "input_y_t1 = tf.placeholder(tf.float32, shape=[FLAGS.batch_size, FLAGS.y_dim])\n",
    "\n",
    "t0_np = np.zeros([FLAGS.batch_size,2],dtype=np.float32); t0_np[:,0] = 1\n",
    "t1_np = np.zeros([FLAGS.batch_size,2],dtype=np.float32); t1_np[:,1] = 1\n",
    "t0_tf = tf.constant(t0_np)\n",
    "t1_tf = tf.constant(t1_np)\n",
    "\n",
    "\n",
    "_,phi_x_t0 = encoder2(input_x_t0,None,None,'encoderX')\n",
    "_,phi_x_t1 = encoder2(input_x_t1,None,None,'encoderX')\n",
    "\n",
    "\n",
    "if AVB:\n",
    "    sampled_z0 = s_encoder(phi_x_t0, input_y_t0, t0_tf)\n",
    "    sampled_z1 = s_encoder(phi_x_t1, input_y_t1, t1_tf)\n",
    "else:\n",
    "    latent_code_t0 = encoder(phi_x_t0, input_y_t0, t0_tf)\n",
    "    latent_code_t1 = encoder(phi_x_t1, input_y_t1, t1_tf)\n",
    "    sampled_z0 = gauss_sampler(latent_code_t0)\n",
    "    sampled_z1 = gauss_sampler(latent_code_t1)\n",
    "\n",
    "nu0_vec = critic(sampled_z0, None, 'KL')\n",
    "nu1_vec = critic(sampled_z1, None, 'KL')\n",
    "\n",
    "loss_fkl = tf.reduce_mean(nu0_vec) - tf.reduce_mean(tf.exp(nu1_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\# Auxiliary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_z_x = generator(input_x)\n",
    "\n",
    "if RDECOMP:\n",
    "    tau_z_x = simple_mlp(sampled_z_x, 1, 'tau') # causal effect\n",
    "    m_z_x = simple_mlp(sampled_z_x, 1, 'm') # expected outcome \n",
    "else:\n",
    "    \n",
    "#     # S-learner\n",
    "#     input_zt0_x = tf.concat([sampled_z_x,t0_tf],axis=1)\n",
    "#     input_zt1_x = tf.concat([sampled_z_x,t1_tf],axis=1)\n",
    "#     mu_0_z_x = simple_mlp(input_zt0_x, 1, 'tau')\n",
    "#     mu_1_z_x = simple_mlp(input_zt1_x, 1, 'tau')\n",
    "#     tau_z_x = mu_1_z_x - mu_0_z_x # causal effect\n",
    "\n",
    "    mu_0_z_x = simple_mlp(sampled_z_x, 1, 'tau0')\n",
    "    mu_1_z_x = simple_mlp(sampled_z_x, 1, 'tau1')\n",
    "    tau_z_x = mu_1_z_x - mu_0_z_x\n",
    "\n",
    "logit_t_z_x = simple_mlp(sampled_z_x, 2, 'ps') \n",
    "e_z_x = tf.reshape(tf.nn.softmax(logit_t_z_x)[:,1],[-1,1]) # propensity score\n",
    "\n",
    "\n",
    "nu0x_vec = critic(sampled_z, input_x, 'gen')\n",
    "nu1x_vec = critic(sampled_z_x, input_x, 'gen')\n",
    "\n",
    "loss_xkl = tf.reduce_mean(nu0x_vec) - tf.reduce_mean(tf.exp(nu1x_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_x = phi_x\n",
    "\n",
    "\n",
    "logit_t_x = qt(feature_x)\n",
    "loglik_qt = tf.nn.softmax_cross_entropy_with_logits_v2(labels=input_t, logits=logit_t_x)\n",
    "loss_qt = tf.reduce_mean(loglik_qt)\n",
    "\n",
    "\n",
    "# continuous outcome\n",
    "# binned multinomial regression ^_^\n",
    "\n",
    "\n",
    "# logit_y_xt = qy(input_x,tf.cast(input_t,tf.float32))\n",
    "logit_y_xt = qy(feature_x,tf.cast(input_t,tf.float32))\n",
    "loglik_qy = tf.nn.softmax_cross_entropy_with_logits(labels=input_ymul, logits=logit_y_xt)\n",
    "loss_qy = tf.reduce_mean(loglik_qy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAM = tf.placeholder(tf.float32)\n",
    "RHO = tf.placeholder(tf.float32)\n",
    "\n",
    "loss_elbo = RHO*loss_r + gamma*loss_e + beta*loss_kl\n",
    "loss_bnice = loss_elbo + LAM*loss_fkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_vars = [v for v in tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('m')]\n",
    "tau_vars = [v for v in tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('tau')]\n",
    "ps_vars = [v for v in tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('ps')]\n",
    "\n",
    "critic_vars = [v for v in tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('critic')]\n",
    "encoder_vars = [v for v in tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('encoder')]\n",
    "gen_vars = [v for v in tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('generator')]\n",
    "\n",
    "elbo_vars = m_vars + tau_vars + ps_vars + encoder_vars\n",
    "\n",
    "qt_vars = [v for v in tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('qt')]\n",
    "qy_vars = [v for v in tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('qy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if l2_reg>0:\n",
    "\n",
    "    l2_loss = 0.\n",
    "    var_list = [elbo_vars, critic_vars, gen_vars]\n",
    "    for vv in var_list:\n",
    "        for v in vv:\n",
    "            if ('bias' in v.name) is False:\n",
    "                l2_loss += tf.reduce_sum(tf.square(v))\n",
    "\n",
    "\n",
    "\n",
    "    l2_loss *= l2_reg\n",
    "\n",
    "    loss_fkl += l2_loss\n",
    "    loss_xkl += l2_loss\n",
    "    loss_bnice += l2_loss\n",
    "\n",
    "\n",
    "loss_bnice += loss_e_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "elbo_lr = .1*learning_rate\n",
    "critic_lr = learning_rate\n",
    "\n",
    "Optimizer = tf.train.AdamOptimizer\n",
    "\n",
    "\n",
    "train_elbo = Optimizer(learning_rate).minimize(loss_elbo,var_list=elbo_vars)   # for encoder/decoder\n",
    "\n",
    "if AVB:\n",
    "    train_critic = Optimizer(critic_lr).minimize(-loss_fkl-loss_xkl-loss_kl, var_list=critic_vars)\n",
    "else:\n",
    "    train_critic = Optimizer(critic_lr).minimize(-loss_fkl-loss_xkl, var_list=critic_vars)\n",
    "\n",
    "train_bnice = Optimizer(learning_rate).minimize(loss_bnice, var_list=elbo_vars)\n",
    "\n",
    "train_aux_qt = Optimizer(learning_rate).minimize(loss_qt, var_list=qt_vars)   # for q(t|x)\n",
    "train_aux_qy = Optimizer(learning_rate).minimize(loss_qy, var_list=qy_vars)   # for q(y|t,x)\n",
    "\n",
    "train_aux_gen = Optimizer(learning_rate).minimize(loss_xkl, var_list=gen_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(Y, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1.0353230010569097, -0.14237197068333618, 3.9263174533843994]\n",
      "PEHE=2.26, CORR=0.48\n",
      "[2, -0.6723648542165757, 0.032553612351417516, 3.5209836959838867]\n",
      "PEHE=1.52, CORR=0.66\n",
      "[3, -1.127568998157978, 0.08769638025760651, 3.4192981719970703]\n",
      "PEHE=1.60, CORR=0.71\n",
      "[4, -1.3220951516628265, 0.15691566908359522, 3.5713720321655273]\n",
      "PEHE=2.08, CORR=0.73\n",
      "[5, -1.4027493510246276, 0.23679066497087475, 3.569343090057373]\n",
      "PEHE=1.74, CORR=0.65\n",
      "[6, -1.4436620024442672, 0.31469964772462844, 3.45459246635437]\n",
      "PEHE=1.52, CORR=0.66\n",
      "[7, -1.467358467578888, 0.39408910441398626, 3.567063093185425]\n",
      "PEHE=1.52, CORR=0.65\n",
      "[8, -1.4798686562776566, 0.47602240672707563, 3.4742825031280518]\n",
      "PEHE=1.58, CORR=0.67\n",
      "[9, -1.491670802116394, 0.5385865451693534, 3.563175916671753]\n",
      "PEHE=1.53, CORR=0.66\n",
      "[10, -1.4978580566644668, 0.5783365457952023, 3.591698169708252]\n",
      "PEHE=1.58, CORR=0.65\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training for encoder/deconder\n",
    "# lam = 0.01\n",
    "\n",
    "max_epoch = 10\n",
    "updates_per_epoch = 1000\n",
    "\n",
    "epoch_record = np.zeros([max_epoch,])\n",
    "\n",
    "for epoch_id in range(max_epoch):\n",
    "    \n",
    "    loss_record = np.zeros([updates_per_epoch,])\n",
    "    loss_xkl_record = np.zeros([updates_per_epoch,])\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    for step in range(updates_per_epoch):\n",
    "\n",
    "        ind = np.random.choice(n_samples,FLAGS.batch_size)\n",
    "        feed_dict = {learning_rate:lr, RHO:rho, LAM:lam}\n",
    "        feed_dict[input_x] = X[ind]\n",
    "        feed_dict[input_y] = Y[ind]\n",
    "        feed_dict[input_t] = T[ind]\n",
    "        \n",
    "        feed_dict[input_ymul] = Ymul[ind]\n",
    "        \n",
    "        ind = np.random.choice(n0,FLAGS.batch_size)\n",
    "        feed_dict[input_x_t0] = X0[ind]\n",
    "        feed_dict[input_y_t0] = Y0[ind]\n",
    "        ind = np.random.choice(n1,FLAGS.batch_size)\n",
    "        feed_dict[input_x_t1] = X1[ind]\n",
    "        feed_dict[input_y_t1] = Y1[ind]\n",
    "        \n",
    "        _,_,_,loss_xkl_val,loss_bnice_val = sess.run([train_critic, train_bnice, train_aux_gen, \\\n",
    "                                        loss_xkl,loss_bnice], feed_dict)\n",
    "\n",
    "\n",
    "        loss_record[step] = loss_bnice_val\n",
    "        loss_xkl_record[step] = loss_xkl_val\n",
    "    \n",
    "    t1 = time()\n",
    "    \n",
    "    klm = np.mean(loss_xkl_record)+1\n",
    "    print([epoch_id+1,np.mean(loss_record),klm,t1-t0])\n",
    "    epoch_record[epoch_id] = np.mean(loss_record)\n",
    "    \n",
    "    tau_hat = estimate_causal_effect(X, n_runs=128, progress=False)\n",
    "\n",
    "    pehe_bnice = eval_pehe(tau_hat, Tau)*y_std\n",
    "    corr_bnice = np.corrcoef(tau_hat.reshape([-1,]),Tau.reshape([-1,]))[0,1]\n",
    "    \n",
    "    print('PEHE=%.2f, CORR=%.2f' % (pehe_bnice, corr_bnice))\n",
    "    \n",
    "_ = plt.plot(epoch_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEHE: 1.58\n",
      "CORR: 0.64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In-sample validation\n",
    "\n",
    "tau_hat = estimate_causal_effect(X, n_runs=128, progress=False)\n",
    "\n",
    "pehe_bnice = eval_pehe(tau_hat, Tau)*y_std\n",
    "corr_bnice = np.corrcoef(tau_hat.reshape([-1,]),Tau.reshape([-1,]))[0,1]\n",
    "\n",
    "# pehe_bnice\n",
    "print('PEHE: %.2f' % pehe_bnice)\n",
    "print('CORR: %.2f' % corr_bnice)\n",
    "\n",
    "_ = plt.plot(Tau,tau_hat,\"s\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "indz = 0\n",
    "\n",
    "\n",
    "z0_val = tf_eval(sampled_z0, n0, {input_x_t0: X0, input_y_t0: Y0})\n",
    "z1_val = tf_eval(sampled_z1, n1, {input_x_t1: X1, input_y_t1: Y1})\n",
    "_ = plt.hist(z0_val[:,indz],bins=20,density=True)\n",
    "_ = plt.hist(z1_val[:,indz],bins=20,density=True)\n",
    "\n",
    "_ = plt.title('B-NICE',fontsize=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
