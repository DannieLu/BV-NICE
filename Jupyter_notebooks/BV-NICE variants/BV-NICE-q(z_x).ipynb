{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\ELBO}{\\text{ELBO}}\n",
    "\\newcommand{\\EE}{\\mathbb{E}}\n",
    "\\newcommand{\\bs}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\ud}{\\,\\text{d}}\n",
    "\\newcommand{\\CN}{\\mathcal{N}}\n",
    "\\newcommand{\\CB}{\\mathcal{B}}\n",
    "\\newcommand{\\aux}{\\text{aux}}\n",
    "\\newcommand{\\do}{do}\n",
    "\\newcommand{\\Xmat}{\\boldsymbol{X}}\n",
    "\\newcommand{\\zv}{\\bs{z}}\n",
    "\\newcommand{\\xv}{\\bs{x}}\n",
    "\\newcommand{\\Xv}{\\bs{X}}\n",
    "\\newcommand{\\Yv}{\\bs{Y}}\n",
    "\\newcommand{\\BD}{\\mathbb{D}}\n",
    "\\newcommand{\\KL}{\\text{KL}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing Variational Neural Inference Causal Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using reformulated ELBO for training\n",
    "  * $m(\\zv) = \\EE[Y|\\zv]$: expected outcome model\n",
    "  * $e(\\zv) = p(t|\\zv)$: propensity score model\n",
    "  * $\\tau(\\zv) = \\EE[Y(1) - Y(0)|\\zv]$: causal effect model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Learning objective:\n",
    "  * Robinson residual: $r(\\zv) = y - m(\\zv) - (t - e(\\zv))\\times\\tau(\\zv)$\n",
    "  * $\\ELBO = \\log e(\\zv) - \\rho r^2(\\zv) - \\BD_{\\KL}(q \\parallel p) - \\lambda \\BD_{\\KL}(q_0 \\parallel q_1)$\n",
    "    * $\\zv \\sim q(\\zv|\\xv)$\n",
    "    * $\\rho$ is the precision parameter\n",
    "    * $\\lambda$ is the balancing regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement=True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l2_reg = 1e-4\n",
    "beta = 1.   # ELBO's KL term\n",
    "gamma = 1.  # PS likelihood\n",
    "rho = 1     # outcome likelihood\n",
    "lam = 1     # imbalance parameter\n",
    "\n",
    "# lr = 1e-3\n",
    "lr = 1e-4\n",
    "# lam = 0\n",
    "\n",
    "ihdp_id = 0\n",
    "# rho = 10\n",
    "\n",
    "# NORM = False\n",
    "NORM = True\n",
    "\n",
    "l2_reg = 0\n",
    "# l2_reg = 1e-3\n",
    "\n",
    "# RDECOMP = False\n",
    "RDECOMP = True\n",
    "\n",
    "# Adversarial VAE\n",
    "# whether to match the aggregated posterior to the prior\n",
    "# AVB = False\n",
    "AVB = True  \n",
    "\n",
    "XONLY = True\n",
    "XONLY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class flags:\n",
    "    \n",
    "    # dim = 2\n",
    "    \n",
    "    x_dim = 25\n",
    "    y_dim = 1\n",
    "    t_dim = 2\n",
    "    M = 10\n",
    "    \n",
    "    hidden_units = 64\n",
    "    hidden_size = 2\n",
    "    \n",
    "    # hidden_size = 1\n",
    "    # hidden_units = 32\n",
    "    \n",
    "    xi_dim = 100\n",
    "    \n",
    "    # optimization\n",
    "    learning_rate = 1e-3 # Base learning rate\n",
    "    lr_decay = 0.999995 # Learning rate decay, applied every step of the optimization\n",
    "    \n",
    "    batch_size = 128 # Batch size during training per GPU\n",
    "    # batch_size = 400\n",
    "    \n",
    "    \n",
    "FLAGS = flags()\n",
    "args = FLAGS\n",
    "\n",
    "DTYPE = tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(t,dim):\n",
    "    \n",
    "    m_samples = t.shape[0]\n",
    "    tt = np.zeros([m_samples,dim])\n",
    "    \n",
    "    for i in range(m_samples):\n",
    "        tt[i,np.int(t[i])] = 1\n",
    "        \n",
    "    return tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     6
    ]
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    y = exp_x / np.sum(exp_x,axis=1,keepdims=True)\n",
    "    # y = y / np.sum(y,axis=1,keepdims=True)\n",
    "    return y\n",
    "\n",
    "def sample_category_logits(logits):\n",
    "    pval = softmax(logits)\n",
    "    n,m = pval.shape\n",
    "    sampl = np.zeros([n,m])\n",
    "    for i in range(pval.shape[0]):\n",
    "        sampl[i,:] = np.random.multinomial(1,pval[i,:])\n",
    "\n",
    "    return sampl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     6,
     40,
     66,
     86,
     105
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# nonlinearity=tf.nn.elu\n",
    "nonlinearity=tf.nn.relu\n",
    "\n",
    "def encoder(input_x,input_y,input_t,name='encoder'):\n",
    "    '''\n",
    "    approximate posterior of z given x,y,t\n",
    "    p(z|x,y,t)\n",
    "    return mean and var of z\n",
    "    \n",
    "    '''\n",
    "    if input_y is None:\n",
    "        input_tensor = input_x\n",
    "    else:\n",
    "        input_tensor = tf.concat([input_x,input_y,input_t],axis=-1)  # cbind \n",
    "    \n",
    "    hidden_units = FLAGS.hidden_units   # size of hidden units in a layer\n",
    "    # latent_size = 2     # dimension of z\n",
    "    \n",
    "    with tf.variable_scope(name,reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        # h2 = tf.layers.dense(h2,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        o = tf.layers.dense(h2,2*FLAGS.hidden_size,kernel_initializer=initializer,activation=None)\n",
    "        # o = tf.layers.dense(h2,2*latent_size,activation=None,use_bias=False)\n",
    "        \n",
    "    # o is the output of encoder\n",
    "    # o[:hidden_size]: mean(z)\n",
    "    # o[hidden_size:]: log var(z)\n",
    "    \n",
    "#     mean = o[:, :FLAGS.hidden_size]\n",
    "#     std = o[:, FLAGS.hidden_size:]\n",
    "#     o = tf.concat([mean,tf.constant(-4+np.zeros([FLAGS.batch_size,FLAGS.hidden_size]),tf.float32)],axis=1)\n",
    "    \n",
    "    # stddev = tf.sqrt(tf.exp(latent_code[:, FLAGS.hidden_size:]))\n",
    "    \n",
    "    return o\n",
    "\n",
    "def s_encoder(input_x,input_y,input_t,name='encoder'):\n",
    "    '''\n",
    "    stocahstic encoder\n",
    "    approximate posterior of z given x,y,t\n",
    "    p(z|x,y,t)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    xi = tf.random.uniform([FLAGS.batch_size,FLAGS.xi_dim], minval=-1,maxval=1)\n",
    "    \n",
    "    if input_y is None:\n",
    "        input_tensor = tf.concat([input_x,xi],axis=-1)\n",
    "    else:\n",
    "        input_tensor = tf.concat([input_x,input_y,input_t,xi],axis=-1)  # cbind \n",
    "    \n",
    "    hidden_units = FLAGS.hidden_units   # size of hidden units in a layer\n",
    "    # latent_size = 2     # dimension of z\n",
    "    \n",
    "    with tf.variable_scope(name,reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        # h2 = tf.layers.dense(h2,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        o = tf.layers.dense(h2,FLAGS.hidden_size,kernel_initializer=initializer,activation=None)\n",
    "        \n",
    "    return o\n",
    "\n",
    "def generator(x, name='generator'):\n",
    "    \n",
    "    '''\n",
    "    Conditional generator for q(z|x)\n",
    "    '''\n",
    "    \n",
    "    hidden_units = FLAGS.hidden_units \n",
    "    \n",
    "    xi = tf.random.uniform([FLAGS.batch_size,FLAGS.xi_dim], minval=-1,maxval=1)\n",
    "    \n",
    "    input_tensor = tf.concat([x,xi],axis=-1)\n",
    "    \n",
    "    with tf.variable_scope('%s' % name,reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        # h2 = tf.layers.dense(h2,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        o = tf.layers.dense(h2,FLAGS.hidden_size,kernel_initializer=initializer,activation=None)\n",
    "    \n",
    "    return o\n",
    "    \n",
    "def critic(z,x,name):\n",
    "    \n",
    "    hidden_units = FLAGS.hidden_units    # size of hidden units in a layer\n",
    "    \n",
    "    if x is None:\n",
    "        input_tensor = z;\n",
    "    else:\n",
    "        input_tensor = tf.concat([x,z],axis=-1)\n",
    "    \n",
    "    with tf.variable_scope('critic-%s' % name,reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        # h2 = tf.layers.dense(h2,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        # o = tf.layers.dense(h2,1,activation=None)\n",
    "        o = tf.layers.dense(h2,1,kernel_initializer=initializer,activation=tf.nn.tanh)\n",
    "        o *= 5\n",
    "        \n",
    "    return o;\n",
    "\n",
    "def encoder2(input_x,input_y,input_t,name='encoder',nonlinearity=tf.nn.elu):\n",
    "    '''\n",
    "    approximate posterior of z given x,y,t\n",
    "    p(z|x,y,t)\n",
    "    return mean and var of z\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if input_y is None or input_t is None:\n",
    "        input_tensor = input_x\n",
    "    else:\n",
    "        input_tensor = tf.concat([input_x,input_y,input_t],axis=-1)  # cbind \n",
    "    \n",
    "    hidden_units = FLAGS.hidden_units   # size of hidden units in a layer\n",
    "    # latent_size = 2     # dimension of z\n",
    "    \n",
    "    with tf.variable_scope(name,reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        # h2 = tf.layers.dense(h2,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        o = tf.layers.dense(h2,2*FLAGS.hidden_size,kernel_initializer=initializer,activation=None)\n",
    "        # o = tf.layers.dense(h2,2*latent_size,activation=None,use_bias=False)\n",
    "        \n",
    "    # o is the output of encoder\n",
    "    # o[:hidden_size]: mean(z)\n",
    "    # o[hidden_size:]: log var(z)\n",
    "    \n",
    "    return o,h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def simple_mlp(x,out_dim,name):\n",
    "    \n",
    "    hidden_units = FLAGS.hidden_units   # size of hidden units in a layer\n",
    "    \n",
    "    input_tensor = x\n",
    "    \n",
    "    with tf.variable_scope('%s' % name,reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        # h2 = tf.layers.dense(h2,hidden_units,kernel_initializer=initializer,activation=nonlinearity)\n",
    "        o = tf.layers.dense(h2,out_dim,kernel_initializer=initializer,activation=None)\n",
    "        \n",
    "    return o;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     20
    ]
   },
   "outputs": [],
   "source": [
    "def qt(input_x,nonlinearity=tf.nn.elu):\n",
    "    '''\n",
    "    Auxiliary treatment predictor\n",
    "    q(t|x)\n",
    "\n",
    "    return: logits\n",
    "    '''\n",
    "\n",
    "    input_tensor = input_x\n",
    "\n",
    "    hidden_units = 64   # size of hidden units in a layer\n",
    "    # latent_size = 2     # dimension of z\n",
    "\n",
    "    with tf.variable_scope('qt',reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,activation=nonlinearity)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,activation=nonlinearity)\n",
    "        o = tf.layers.dense(h2,2,activation=None)\n",
    "\n",
    "    return o\n",
    "\n",
    "def qy(input_x,input_t,nonlinearity=tf.nn.elu):\n",
    "    '''\n",
    "    Auxiliary outcome predictor\n",
    "    q(y|x,t)\n",
    "\n",
    "    return: logits\n",
    "    '''\n",
    "\n",
    "    input_tensor = tf.concat([input_x,input_t],axis=-1)\n",
    "\n",
    "    hidden_units = 64   # size of hidden units in a layer\n",
    "    # latent_size = 2     # dimension of z\n",
    "\n",
    "    with tf.variable_scope('qy',reuse=tf.AUTO_REUSE):\n",
    "        h1 = tf.layers.dense(input_tensor,hidden_units,activation=nonlinearity)\n",
    "        h2 = tf.layers.dense(h1,hidden_units,activation=nonlinearity)\n",
    "        o = tf.layers.dense(h2,FLAGS.M,activation=None)\n",
    "\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def eval_pehe(tau_hat,tau):\n",
    "    return np.sqrt(np.mean(np.square(tau-tau_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VAE utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_kl_cost(mean,stddev, epsilon=1e-8):                      \n",
    "    '''\n",
    "    Compute the KL-divergence btw q(z|x) and p(z)\n",
    "    \n",
    "    KL(q||p)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return tf.reduce_sum(0.5 * (tf.square(mean) + tf.square(stddev) -\n",
    "                                2.0 * tf.log(stddev + epsilon) - 1.0), axis=1)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0,
     3,
     12,
     40
    ]
   },
   "outputs": [],
   "source": [
    "def int_shape(x):\n",
    "    return list(map(int, x.get_shape()))\n",
    "\n",
    "def print_shape(x,varname='variable'):\n",
    "    if x is None:\n",
    "        print('%s size: None' % (varname))\n",
    "        return\n",
    "    x_shape = x.shape.as_list()\n",
    "    # print('%s size: [%d,%d,%d]' % (varname,x_shape[1],x_shape[2],x_shape[3]))\n",
    "    print(varname,end=': ')\n",
    "    print(x_shape)\n",
    "\n",
    "def tf_eval(tf_tensor,n_samples,feed_dict=None):\n",
    "    \n",
    "    MLOOP = np.int(np.ceil(n_samples/FLAGS.batch_size))\n",
    "    \n",
    "    dd = tf_tensor.shape.as_list()[1:]\n",
    "    dd.insert(0,n_samples)\n",
    "    \n",
    "    x = np.zeros(dd)\n",
    "    \n",
    "    for mloop in range(MLOOP):\n",
    "        \n",
    "        st = mloop*FLAGS.batch_size\n",
    "        ed = min((mloop+1)*FLAGS.batch_size, n_samples)\n",
    "        \n",
    "        if feed_dict is not None:\n",
    "            feed_dict_i = dict()\n",
    "            for key in feed_dict.keys():\n",
    "                feed_dict_i[key] = np.random.randn(*int_shape(key))\n",
    "                feed_dict_i[key][:ed-st] = feed_dict[key][st:ed]\n",
    "            y = sess.run(tf_tensor,feed_dict=feed_dict_i)\n",
    "        else:\n",
    "            y = sess.run(tf_tensor)\n",
    "        \n",
    "        # print([st,ed])\n",
    "        x[st:ed] = y[:ed-st]\n",
    "    \n",
    "    return x\n",
    "\n",
    "def tf_eval_list(tf_tensor_list,n_samples,feed_dict=None):\n",
    "    \n",
    "    if isinstance(tf_tensor_list, list)==False:\n",
    "        print('Input not a list')\n",
    "        return None\n",
    "    \n",
    "    MLOOP = np.int(np.ceil(n_samples/FLAGS.batch_size))\n",
    "    \n",
    "    res = dict()\n",
    "\n",
    "    for key in tf_tensor_list:\n",
    "        dd = key.shape.as_list()[1:]\n",
    "        dd.insert(0,n_samples)\n",
    "        res[key] = np.zeros(dd)\n",
    "    \n",
    "    for mloop in range(MLOOP):\n",
    "        \n",
    "        st = mloop*FLAGS.batch_size\n",
    "        ed = min((mloop+1)*FLAGS.batch_size,n_samples)\n",
    "        \n",
    "        if feed_dict is not None:\n",
    "            feed_dict_i = dict()\n",
    "            for key in feed_dict.keys():\n",
    "                feed_dict_i[key] = np.random.randn(*int_shape(key))\n",
    "                feed_dict_i[key][:ed-st] = feed_dict[key][st:ed]\n",
    "            # print(feed_dict_i)\n",
    "            y = sess.run(tf_tensor_list,feed_dict=feed_dict_i)\n",
    "        else:\n",
    "            y = sess.run(tf_tensor_list)\n",
    "        \n",
    "        for i in range(len(tf_tensor_list)):\n",
    "            res[tf_tensor_list[i]][st:ed] = y[i][:ed-st]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [],
   "source": [
    "def ymul2y(yy,Yq):\n",
    "    \n",
    "    m_samples = yy.shape[0]\n",
    "    y = np.zeros([m_samples,1])\n",
    "    \n",
    "    for i in range(m_samples):\n",
    "        j = np.where(yy[i,:])[0][0]\n",
    "        y[i] = Yq[j] + np.random.rand()*(Yq[j+1]-Yq[j]) \n",
    "        \n",
    "    return y\n",
    "\n",
    "def estimate_causal_effect(xx,n_runs=1,progress=False):\n",
    "    \n",
    "    m_samples = xx.shape[0]\n",
    "    tau_sum = np.zeros([m_samples,1])\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        \n",
    "        if progress:\n",
    "            print('Computing %d / %d ...' % (i+1, n_runs))\n",
    "        \n",
    "        # zz = tf_eval(sampled_z_x, m_samples, {input_x: xx})\n",
    "        tau_val = tf_eval(tau_z, m_samples, {sampled_z: zz}) # this line\n",
    "        # tau_val = tf_eval(tau_z_x, m_samples, {input_x: xx})\n",
    "        \n",
    "        tau_sum += tau_val\n",
    "        \n",
    "    return tau_sum/n_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0,
     28,
     34
    ]
   },
   "outputs": [],
   "source": [
    "def estimate_ps(xx,n_runs=1,progress=False):\n",
    "    \n",
    "    '''\n",
    "    xx: confounder proxies\n",
    "    to: observed treatment\n",
    "    '''\n",
    "    \n",
    "    m_samples = xx.shape[0]\n",
    "    # tau_sum = np.zeros([m_samples,1])\n",
    "    \n",
    "    # print(m_samples)\n",
    "    \n",
    "    yy_mdl = np.zeros([m_samples,n_runs])\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        \n",
    "        if progress:\n",
    "            print('Computing %d / %d ...' % (i+1, n_runs))\n",
    "        \n",
    "#         res = tf_eval_list([tau_z_x, m_z_x, e_z_x], m_samples, {input_x: xx})\n",
    "#         m_val = res[m_z_x]\n",
    "#         tau_val = res[tau_z_x]\n",
    "#         e_val = res[e_z_x]\n",
    "        e_val = tf_eval(e_z_x, m_samples, {input_x: xx})\n",
    "        yy_mdl[:,i] = e_val.reshape([-1,])\n",
    "        \n",
    "    return yy_mdl\n",
    "\n",
    "def eval_nll_ps(X_val,T_val,n_runs=1):\n",
    "    ps_val = np.mean(estimate_ps(X_val,n_runs=100),axis=1)\n",
    "    loglik_ps_val = np.log(T_val[:,0]*(1-ps_val)+T_val[:,1]*ps_val)\n",
    "    ps_nll_val = -np.mean(loglik_ps_val)\n",
    "    return ps_nll_val\n",
    "\n",
    "def eval_nll_ps_x(X_val,T_val):\n",
    "    \n",
    "    m_samples = X_val.shape[0]\n",
    "    ps_val = tf_eval(e_x, m_samples, {input_x: X_val}).reshape([-1,])\n",
    "    loglik_ps_val = np.log(T_val[:,0]*(1-ps_val)+T_val[:,1]*ps_val)\n",
    "    ps_nll_val = -np.mean(loglik_ps_val)\n",
    "    \n",
    "    return ps_nll_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_ihdp(trial_id=0,filepath='./data/',istrain=True):\n",
    "    \n",
    "    if istrain:\n",
    "        data_file = filepath+'ihdp_npci_1-1000.train.npz'\n",
    "    else:\n",
    "        data_file = filepath+'ihdp_npci_1-1000.test.npz'\n",
    "        \n",
    "    data = np.load(data_file)\n",
    "    \n",
    "    x = data['x'][:,:,trial_id]\n",
    "    y = data['yf'][:,trial_id]\n",
    "    t = data['t'][:,trial_id]\n",
    "    ycf = data['ycf'][:,trial_id]\n",
    "    mu0 = data['mu0'][:,trial_id]\n",
    "    mu1 = data['mu1'][:,trial_id]\n",
    "    \n",
    "    return x,y,t,ycf,mu0,mu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading normalized IHDP example\n",
    "data = np.load('../ihdp_example.npy',allow_pickle=True).item()\n",
    "X = data['X'] # Confounder proxiess\n",
    "Y = data['Y'] # Factual outcomes\n",
    "T = data['T'] # Treatment\n",
    "Tau = data['Tau']  # Ground truth ITE\n",
    "\n",
    "n_samples = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "\n",
    "y_std = 1.\n",
    "\n",
    "if NORM:\n",
    "    y_m = np.mean(Y)\n",
    "    y_std = np.std(Y)\n",
    "\n",
    "    Y = (Y-y_m)/y_std\n",
    "\n",
    "    Tau = Tau/y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break Y into M bins\n",
    "\n",
    "M = FLAGS.M\n",
    "q = np.linspace(0,1,M+1)\n",
    "\n",
    "Yq = np.quantile(Y,q)\n",
    "\n",
    "Yqq = Yq.copy()\n",
    "Yqq[0] = -1e10\n",
    "Yqq[-1] = 1e10\n",
    "\n",
    "n_samples = Y.shape[0]\n",
    "Ymul = np.zeros([n_samples,M])\n",
    "\n",
    "for i in range(n_samples):\n",
    "    for j in range(M):\n",
    "        if Y[i]>= Yqq[j] and Y[i]<Yqq[j+1]:\n",
    "            Ymul[i,j] = 1\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_train = 0.7\n",
    "\n",
    "n_train_samples = int(np.ceil(prob_train*n_samples))\n",
    "\n",
    "shuff_idx = np.array(range(n_samples))\n",
    "# Shuffle the indices\n",
    "# np.random.shuffle(shuff_idx)\n",
    "\n",
    "train_idx = shuff_idx[:n_train_samples]\n",
    "val_idx = shuff_idx[n_train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X[val_idx]\n",
    "Y_val = Y[val_idx]\n",
    "T_val = T[val_idx]\n",
    "\n",
    "X = X[train_idx]\n",
    "Y = Y[train_idx]\n",
    "T = T[train_idx]\n",
    "Ymul = Ymul[train_idx]\n",
    "\n",
    "n_samples = n_train_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tau_val = Tau[val_idx]\n",
    "Tau = Tau[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_ind = T[:,1]==1   # find which column has the treatment == 1\n",
    "t0_ind = T[:,0]==1 \n",
    "\n",
    "n0 = np.sum(t0_ind)\n",
    "n1 = np.sum(t1_ind)\n",
    "\n",
    "X0 = X[t0_ind]\n",
    "X1 = X[t1_ind]\n",
    "\n",
    "Y0 = Y[t0_ind]\n",
    "Y1 = Y[t1_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def gauss_sampler(latent_code=None):\n",
    "    epsilon = tf.random_normal([FLAGS.batch_size, FLAGS.hidden_size])\n",
    "    \n",
    "    # input_tensor = tf.cast(input_tensor,tf.float32)\n",
    "    # input_t = tf.cast(input_t, tf.float32)\n",
    "    \n",
    "    # If input is none, sample z from prior\n",
    "    # input_tensor[:half]: mean, input_tensor[half:]: log var,\n",
    "    if latent_code is None:\n",
    "        mean = None\n",
    "        stddev = None\n",
    "        input_sample = epsilon\n",
    "    else:\n",
    "        mean = latent_code[:, :FLAGS.hidden_size]\n",
    "        stddev = tf.sqrt(tf.exp(latent_code[:, FLAGS.hidden_size:]))\n",
    "        input_sample = mean + epsilon * stddev\n",
    "        \n",
    "    return input_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-481aa1bec777>:123: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "input_x = tf.placeholder(tf.float32, shape=[FLAGS.batch_size, FLAGS.x_dim])\n",
    "input_t = tf.placeholder(tf.int32, shape=[FLAGS.batch_size, FLAGS.t_dim])\n",
    "input_y = tf.placeholder(tf.float32, shape=[FLAGS.batch_size, FLAGS.y_dim])\n",
    "input_ymul = tf.placeholder(tf.int32, shape=[FLAGS.batch_size, FLAGS.M])\n",
    "\n",
    "_,phi_x = encoder2(input_x,None,None,'encoderX')\n",
    "\n",
    "if AVB:\n",
    "    if XONLY: sampled_z = s_encoder(phi_x, None, None)  # q(z|x)\n",
    "    else: sampled_z = s_encoder(phi_x, input_y, tf.cast(input_t,tf.float32))  # q(z|x)\n",
    "    \n",
    "    nu_q_vec = critic(sampled_z, None, 'ELBO')\n",
    "    nu_p_vec = critic(tf.random.normal([FLAGS.batch_size, FLAGS.hidden_size]), None, 'ELBO')\n",
    "\n",
    "    loss_kl = tf.reduce_mean(nu_q_vec) - tf.reduce_mean(tf.exp(nu_p_vec))\n",
    "    \n",
    "else:\n",
    "    if XONLY: latent_code = encoder(phi_x, None, None)  # q(z|x)\n",
    "    else: latent_code = encoder(phi_x, input_y, tf.cast(input_t,tf.float32))  # q(z|x)\n",
    "    sampled_z = gauss_sampler(latent_code)\n",
    "    \n",
    "    mean = latent_code[:,:FLAGS.hidden_size]\n",
    "    stddev = tf.sqrt(tf.exp(latent_code[:,FLAGS.hidden_size:]))\n",
    "    loss_kl_vec = get_kl_cost(mean,stddev)\n",
    "    loss_kl = tf.reduce_mean(loss_kl_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\# propensity model absed on $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-32-10af06cafe52>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logit_t_x = simple_mlp(input_x, 2, 'ps_x') # propensity score\n",
    "e_x = tf.reshape(tf.nn.softmax(logit_t_x)[:,1],[-1,1])\n",
    "\n",
    "loss_e_x_vec = tf.nn.softmax_cross_entropy_with_logits(labels=input_t, logits=logit_t_x)\n",
    "loss_e_x = tf.reduce_mean(loss_e_x_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\# Robinson residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if RDECOMP:\n",
    "\n",
    "    m_z = simple_mlp(sampled_z, 1, 'm') # expected outcome \n",
    "    tau_z = simple_mlp(sampled_z, 1, 'tau') # causal effect\n",
    "    logit_t_z = simple_mlp(sampled_z, 2, 'ps') # propensity score\n",
    "    e_z = tf.reshape(tf.nn.softmax(logit_t_z)[:,1],[-1,1])\n",
    "\n",
    "    loss_e_vec = tf.nn.softmax_cross_entropy_with_logits(labels=input_t, logits=logit_t_z)\n",
    "    loss_e = tf.reduce_mean(loss_e_vec)\n",
    "\n",
    "    # Continuous outcome\n",
    "    input_t_bin = tf.reshape(tf.cast(input_t[:,1],dtype=tf.float32),[-1,1])\n",
    "    robinson_res = (input_y - m_z) - (tf.cast(input_t_bin, dtype=tf.float32) - e_z) * tau_z\n",
    "\n",
    "    loss_r = tf.reduce_mean(tf.square(robinson_res))\n",
    "\n",
    "    loss_m = tf.reduce_mean(tf.square(input_y - m_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\# Regular decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RDECOMP is False:\n",
    "\n",
    "\n",
    "#     # S-learner\n",
    "#     input_zt = tf.concat([sampled_z,tf.cast(input_t,tf.float32)],axis=1)\n",
    "#     mu_t_z = simple_mlp(input_zt, 1, 'tau')\n",
    "#     t0_np = np.zeros([FLAGS.batch_size,2],dtype=np.int32); t0_np[:,0] = 1\n",
    "#     t1_np = np.zeros([FLAGS.batch_size,2],dtype=np.int32); t1_np[:,1] = 1\n",
    "#     t0_tf = tf.constant(t0_np,tf.float32)\n",
    "#     t1_tf = tf.constant(t1_np,tf.float32)\n",
    "#     input_zt0 = tf.concat([sampled_z,t0_tf],axis=1)\n",
    "#     input_zt1 = tf.concat([sampled_z,t1_tf],axis=1)\n",
    "#     mu_0_z = simple_mlp(input_zt0, 1, 'tau')\n",
    "#     mu_1_z = simple_mlp(input_zt1, 1, 'tau')\n",
    "    \n",
    "    \n",
    "    # T-learner\n",
    "    mu_0_z = simple_mlp(sampled_z, 1, 'tau0')\n",
    "    mu_1_z = simple_mlp(sampled_z, 1, 'tau1')\n",
    "    input_t_bin = tf.reshape(tf.cast(input_t[:,1],dtype=tf.float32),[-1,1])\n",
    "    mu_t_z = input_t_bin*mu_1_z + (1-input_t_bin)*mu_0_z\n",
    "    \n",
    "    \n",
    "    tau_z = mu_1_z - mu_0_z # causal effect\n",
    "\n",
    "    logit_t_z = simple_mlp(sampled_z, 2, 'ps') # propensity score\n",
    "    e_z = tf.reshape(tf.nn.softmax(logit_t_z)[:,1],[-1,1])\n",
    "\n",
    "    loss_e_vec = tf.nn.softmax_cross_entropy_with_logits(labels=input_t, logits=logit_t_z)\n",
    "    loss_e = tf.reduce_mean(loss_e_vec)\n",
    "\n",
    "    robinson_res = input_y - mu_t_z\n",
    "\n",
    "    loss_r = tf.reduce_mean(tf.square(robinson_res))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\# Balancing KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x_t0 = tf.placeholder(tf.float32, shape=[FLAGS.batch_size, FLAGS.x_dim])\n",
    "input_x_t1 = tf.placeholder(tf.float32, shape=[FLAGS.batch_size, FLAGS.x_dim])\n",
    "input_y_t0 = tf.placeholder(tf.float32, shape=[FLAGS.batch_size, FLAGS.y_dim])\n",
    "input_y_t1 = tf.placeholder(tf.float32, shape=[FLAGS.batch_size, FLAGS.y_dim])\n",
    "\n",
    "t0_np = np.zeros([FLAGS.batch_size,2],dtype=np.float32); t0_np[:,0] = 1\n",
    "t1_np = np.zeros([FLAGS.batch_size,2],dtype=np.float32); t1_np[:,1] = 1\n",
    "t0_tf = tf.constant(t0_np)\n",
    "t1_tf = tf.constant(t1_np)\n",
    "\n",
    "_,phi_x_t0 = encoder2(input_x_t0,None,None,'encoderX')\n",
    "_,phi_x_t1 = encoder2(input_x_t1,None,None,'encoderX')\n",
    "\n",
    "\n",
    "if AVB:\n",
    "    if XONLY:\n",
    "        sampled_z0 = s_encoder(phi_x_t0, None, None)\n",
    "        sampled_z1 = s_encoder(phi_x_t1, None, None)\n",
    "    else:\n",
    "        sampled_z0 = s_encoder(phi_x_t0, input_y_t0, t0_tf)\n",
    "        sampled_z1 = s_encoder(phi_x_t1, input_y_t1, t1_tf)\n",
    "else:\n",
    "    if XONLY:\n",
    "        latent_code_t0 = encoder(phi_x_t0, None, None)\n",
    "        latent_code_t1 = encoder(phi_x_t1, None, None)\n",
    "    else:\n",
    "        latent_code_t0 = encoder(phi_x_t0, input_y_t0, t0_tf)\n",
    "        latent_code_t1 = encoder(phi_x_t1, input_y_t1, t1_tf)\n",
    "    sampled_z0 = gauss_sampler(latent_code_t0)\n",
    "    sampled_z1 = gauss_sampler(latent_code_t1)\n",
    "\n",
    "\n",
    "nu0_vec = critic(sampled_z0, None, 'KL')\n",
    "nu1_vec = critic(sampled_z1, None, 'KL')\n",
    "\n",
    "loss_fkl = tf.reduce_mean(nu0_vec) - tf.reduce_mean(tf.exp(nu1_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \\# Auxiliary models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_z_x = generator(input_x)\n",
    "\n",
    "if RDECOMP:\n",
    "    tau_z_x = simple_mlp(sampled_z_x, 1, 'tau') # causal effect\n",
    "    m_z_x = simple_mlp(sampled_z_x, 1, 'm') # expected outcome \n",
    "else:\n",
    "    \n",
    "#     # S-learner\n",
    "#     input_zt0_x = tf.concat([sampled_z_x,t0_tf],axis=1)\n",
    "#     input_zt1_x = tf.concat([sampled_z_x,t1_tf],axis=1)\n",
    "#     mu_0_z_x = simple_mlp(input_zt0_x, 1, 'tau')\n",
    "#     mu_1_z_x = simple_mlp(input_zt1_x, 1, 'tau')\n",
    "#     tau_z_x = mu_1_z_x - mu_0_z_x # causal effect\n",
    "\n",
    "    mu_0_z_x = simple_mlp(sampled_z_x, 1, 'tau0')\n",
    "    mu_1_z_x = simple_mlp(sampled_z_x, 1, 'tau1')\n",
    "    tau_z_x = mu_1_z_x - mu_0_z_x\n",
    "\n",
    "logit_t_z_x = simple_mlp(sampled_z_x, 2, 'ps') \n",
    "e_z_x = tf.reshape(tf.nn.softmax(logit_t_z_x)[:,1],[-1,1]) # propensity score\n",
    "\n",
    "\n",
    "nu0x_vec = critic(sampled_z, input_x, 'gen')\n",
    "nu1x_vec = critic(sampled_z_x, input_x, 'gen')\n",
    "\n",
    "loss_xkl = tf.reduce_mean(nu0x_vec) - tf.reduce_mean(tf.exp(nu1x_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_x = phi_x\n",
    "\n",
    "\n",
    "logit_t_x = qt(feature_x)\n",
    "loglik_qt = tf.nn.softmax_cross_entropy_with_logits_v2(labels=input_t, logits=logit_t_x)\n",
    "loss_qt = tf.reduce_mean(loglik_qt)\n",
    "# continuous outcome\n",
    "# binned multinomial regression ^_^\n",
    "\n",
    "\n",
    "logit_y_xt = qy(feature_x,tf.cast(input_t,tf.float32))\n",
    "loglik_qy = tf.nn.softmax_cross_entropy_with_logits(labels=input_ymul, logits=logit_y_xt)\n",
    "loss_qy = tf.reduce_mean(loglik_qy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAM = tf.placeholder(tf.float32)\n",
    "RHO = tf.placeholder(tf.float32)\n",
    "\n",
    "loss_elbo = RHO*loss_r + gamma*loss_e + beta*loss_kl\n",
    "loss_bnice = loss_elbo + LAM*loss_fkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_vars = [v for v in tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('m')]\n",
    "tau_vars = [v for v in tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('tau')]\n",
    "ps_vars = [v for v in tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('ps')]\n",
    "\n",
    "# critic_vars = [v for v in tf.get_collection(\n",
    "#     tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('critic-KL')]\n",
    "critic_vars = [v for v in tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('critic')]\n",
    "encoder_vars = [v for v in tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('encoder')]\n",
    "gen_vars = [v for v in tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('generator')]\n",
    "\n",
    "# elbo_vars = m_vars + tau_vars + ps_vars\n",
    "elbo_vars = m_vars + tau_vars + ps_vars + encoder_vars\n",
    "\n",
    "qt_vars = [v for v in tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('qt')]\n",
    "qy_vars = [v for v in tf.get_collection(\n",
    "    tf.GraphKeys.TRAINABLE_VARIABLES) if v.name.startswith('qy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if l2_reg>0:\n",
    "\n",
    "    l2_loss = 0.\n",
    "    var_list = [elbo_vars, critic_vars, gen_vars]\n",
    "    for vv in var_list:\n",
    "        for v in vv:\n",
    "            if ('bias' in v.name) is False:\n",
    "                l2_loss += tf.reduce_sum(tf.square(v))\n",
    "\n",
    "\n",
    "\n",
    "    l2_loss *= l2_reg\n",
    "\n",
    "    loss_fkl += l2_loss\n",
    "    loss_xkl += l2_loss\n",
    "    loss_bnice += l2_loss\n",
    "\n",
    "\n",
    "loss_bnice += loss_e_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "elbo_lr = .1*learning_rate\n",
    "critic_lr = learning_rate\n",
    "\n",
    "Optimizer = tf.train.AdamOptimizer\n",
    "# Optimizer = tf.train.RMSPropOptimizer\n",
    "\n",
    "\n",
    "train_elbo = Optimizer(learning_rate).minimize(loss_elbo,var_list=elbo_vars)   # for encoder/decoder\n",
    "# train_critic = tf.train.AdamOptimizer(critic_lr).minimize(-loss_fkl, var_list=critic_vars)\n",
    "if AVB:\n",
    "    train_critic = Optimizer(critic_lr).minimize(-loss_fkl-loss_xkl-loss_kl, var_list=critic_vars)\n",
    "else:\n",
    "    train_critic = Optimizer(critic_lr).minimize(-loss_fkl-loss_xkl, var_list=critic_vars)\n",
    "\n",
    "train_bnice = Optimizer(learning_rate).minimize(loss_bnice, var_list=elbo_vars)\n",
    "\n",
    "train_aux_qt = Optimizer(learning_rate).minimize(loss_qt, var_list=qt_vars)   # for q(t|x)\n",
    "train_aux_qy = Optimizer(learning_rate).minimize(loss_qy, var_list=qy_vars)   # for q(y|t,x)\n",
    "\n",
    "train_aux_gen = Optimizer(learning_rate).minimize(loss_xkl, var_list=gen_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critic_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(Y, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0.7112258899211884, -0.46163112834095954, 4.002407550811768]\n",
      "PEHE=1.41, CORR=0.29\n",
      "[2, -0.7041922469139099, 0.016403754055499986, 3.6349048614501953]\n",
      "PEHE=0.98, CORR=0.42\n",
      "[3, -1.135898557126522, 0.014679739713668871, 3.4901602268218994]\n",
      "PEHE=0.95, CORR=0.71\n",
      "[4, -1.328780823945999, 0.03495693105459208, 3.6245474815368652]\n",
      "PEHE=0.93, CORR=0.80\n",
      "[5, -1.4135482244491577, 0.10398260891437527, 3.625148057937622]\n",
      "PEHE=1.06, CORR=0.81\n",
      "[6, -1.4528890280723572, 0.13812440231442447, 3.449954032897949]\n",
      "PEHE=1.13, CORR=0.81\n",
      "[7, -1.4756464225053787, 0.18926314580440518, 3.6054766178131104]\n",
      "PEHE=0.88, CORR=0.72\n",
      "[8, -1.4900938521623612, 0.20769159895181655, 3.5240437984466553]\n",
      "PEHE=0.99, CORR=0.50\n",
      "[9, -1.492479257583618, 0.24155121585726735, 3.5799155235290527]\n",
      "PEHE=1.09, CORR=0.73\n",
      "[10, -1.5020298686027527, 0.23873753201961523, 3.634438991546631]\n",
      "PEHE=1.05, CORR=0.66\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training for encoder/deconder\n",
    "# lam = 0.01\n",
    "\n",
    "max_epoch = 10\n",
    "updates_per_epoch = 1000\n",
    "\n",
    "epoch_record = np.zeros([max_epoch,])\n",
    "\n",
    "for epoch_id in range(max_epoch):\n",
    "    \n",
    "    loss_record = np.zeros([updates_per_epoch,])\n",
    "    loss_xkl_record = np.zeros([updates_per_epoch,])\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    for step in range(updates_per_epoch):\n",
    "\n",
    "        # n_samples\n",
    "        ind = np.random.choice(n_samples,FLAGS.batch_size)\n",
    "        # ind\n",
    "        feed_dict = {learning_rate:lr, RHO:rho, LAM:lam}\n",
    "        feed_dict[input_x] = X[ind]\n",
    "        feed_dict[input_y] = Y[ind]\n",
    "        feed_dict[input_t] = T[ind]\n",
    "        \n",
    "        feed_dict[input_ymul] = Ymul[ind]\n",
    "        \n",
    "        ind = np.random.choice(n0,FLAGS.batch_size)\n",
    "        feed_dict[input_x_t0] = X0[ind]\n",
    "        feed_dict[input_y_t0] = Y0[ind]\n",
    "        ind = np.random.choice(n1,FLAGS.batch_size)\n",
    "        feed_dict[input_x_t1] = X1[ind]\n",
    "        feed_dict[input_y_t1] = Y1[ind]\n",
    "        \n",
    "#         _,_,loss_qt_val,loss_qy_val = sess.run([train_aux_qt, train_aux_qy, loss_qt, loss_qy], feed_dict)\n",
    "#         loss_record[step] = loss_qt_val + loss_qy_val\n",
    "        \n",
    "#         _,loss_elbo_val = sess.run([train_elbo, loss_elbo], feed_dict)\n",
    "#         loss_record[step] = loss_elbo_val\n",
    "        \n",
    "#         _,_,loss_bnice_val = sess.run([train_critic, train_bnice, loss_bnice], feed_dict)\n",
    "#         loss_record[step] = loss_bnice_val\n",
    "\n",
    "#         _,_,loss_xkl_val,loss_bnice_val = sess.run([train_critic, train_aux_gen, \\\n",
    "#                                         loss_xkl,loss_bnice], feed_dict)\n",
    "\n",
    "\n",
    "        _,_,_,loss_xkl_val,loss_bnice_val = sess.run([train_critic, train_bnice, train_aux_gen, \\\n",
    "                                        loss_xkl,loss_bnice], feed_dict)\n",
    "    \n",
    "\n",
    "        loss_record[step] = loss_bnice_val\n",
    "        loss_xkl_record[step] = loss_xkl_val\n",
    "    \n",
    "    t1 = time()\n",
    "    \n",
    "    klm = np.mean(loss_xkl_record)+1\n",
    "    print([epoch_id+1,np.mean(loss_record),klm,t1-t0])\n",
    "    epoch_record[epoch_id] = np.mean(loss_record)\n",
    "    \n",
    "    \n",
    "    tau_hat = estimate_causal_effect(X, n_runs=128, progress=False)\n",
    "\n",
    "    pehe_bnice = eval_pehe(tau_hat, Tau)*y_std\n",
    "    corr_bnice = np.corrcoef(tau_hat.reshape([-1,]),Tau.reshape([-1,]))[0,1]\n",
    "    \n",
    "    print('PEHE=%.2f, CORR=%.2f' % (pehe_bnice, corr_bnice))\n",
    "    \n",
    "_ = plt.plot(epoch_record)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
